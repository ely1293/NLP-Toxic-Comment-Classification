{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "#preprocessing\n",
    "import re\n",
    "import contractions\n",
    "import unidecode\n",
    "from gensim.parsing.preprocessing import remove_stopwords, STOPWORDS\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load subreddit files into dictionary of dfs\n",
    "csvs = [x for x in os.listdir('.') if x.endswith('.pkl')]\n",
    "fns = [os.path.splitext(os.path.basename(x))[0] for x in csvs]\n",
    "\n",
    "d = {}\n",
    "for i in range(len(fns)):\n",
    "    d[fns[i]] = pd.read_pickle(csvs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "KotakuInAction = pd.DataFrame(d['KotakuInAction'], columns=['comment_text'])\n",
    "MensRights = pd.DataFrame(d['MensRights'], columns=['comment_text'])\n",
    "NoFap = pd.DataFrame(d['NoFap'], columns=['comment_text'])\n",
    "TrollXChromosomes = pd.DataFrame(d['TrollXChromosomes'], columns=['comment_text'])\n",
    "TumblrInAction = pd.DataFrame(d['TumblrInAction'], columns=['comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     comment_text\n",
      "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          [Humor] There's two kinds of people...\n",
      "1  This is what naturally hitting /r/all looks like.\\n\\n\\#nosticky \\#jealous\\n\\nSpezit: WE'RE NUMBER 1, WE'RE NUMBER 1\\n\\nSpezit 2, report boogaloo:\\n\\n> Hessmix: much cuck, so wow\\n> \\n> ShadistsReddit: I'm not going to say he's a cuck; but...\\n> \\n> Limon_Lime: rightest cuck\\n> \\n> Raraara: Biggest cuck in the universe\\n> \\n> user reports:\\n> \\n> 1: for a repost from r/tia! congrats saint! you made it! came a long way from being retarded in agg!\\n> \\n> 1: leftist cucks\\n> \\n> 1: ummm... okay.\\n\\nLovin' that smarm, people!\n",
      "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Well at least we can confirm that one of the two wasn't self diagnosed.\n",
      "3                                                                                                                                                                                                                                                                                                                                                                         ahaha yeah. I have severe OCD and it's really hellish. I don't care if a sweater is making a joke. The existence of the sweater doesn't help nor hinder the condition. \n",
      "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                            I've got twenty bucks that says special snow flake doesn't have OCD.\n",
      "                                                                                                                                                                                                                                                                                                                                                                                  comment_text\n",
      "0                                                                                                                                                                                                                                                                                                                                                            How to get banned from r/Feminism\n",
      "1  This kind of post is normally removed as it violates our subreddit policies. However, I have been travelling and didn't see it. At this point, with the number of participants and comments, removing it would also have a detrimental effect to that valuable discussion. As a result, I will be leaving this up.\\n\\nThose who continue to report it won't be satisfied with the response.\n",
      "2                                                                                                                                                                                                                                                                                         It looks like they are behind on banning people. Usually [this](http://imgur.com/MAqsCnN) is enough.\n",
      "3                                                                                                                                                                                                                                                                                                          \"Feeling safe\" is how we ended up with the TSA and their useless security theatre. \n",
      "4                                                                                                                                                                                                                                                                                                                                                                                    [deleted]\n",
      "                                                                   comment_text\n",
      "0                                                              wait a minute...\n",
      "1                                      Guess I can't talk to my parents anymore\n",
      "2                                              I'M FROM SPAIN, I HATE THIS SHIT\n",
      "3                      A lot of spanish-speaking latinamerica is white as well \n",
      "4  Reminds me of the post saying that Spanish is a language, not a nationality.\n",
      "                                                                                                                                                                                                                                                                                                                                                            comment_text\n",
      "0                                                                                                                                                                                                                                                                                                                            I don't know why this made me laugh so hard\n",
      "1  I’ve heard so many stories about people catching Corona at religious services.\\n\\nIt’s almost like mass gatherings during a pandemic... are bad.\\n\\nIt’s possible to worship at home in a private way that doesn’t put others at risk. If your relationship with your God is in danger because you can’t go to church every week, maybe your faith ain’t that strong.\n",
      "2                                                                                                                                                                                                                                                                                                The mental image of 600,000 babies piled behind a debate podium got me.\n",
      "3                                                                                                                                                                                                                                                                                                                                                              [deleted]\n",
      "4                                                                                                                                                    Because praying at home wont kill you, but a home abortion could. And how many people have died of COVID19 by going to church, as opposed to going to their doctor for a safe procedure. Why are people so stupid??\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "subreddits = [KotakuInAction, MensRights, NoFap, TumblrInAction, TrollXChromosomes]\n",
    "\n",
    "for subreddit in subreddits:\n",
    "    print(subreddit.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove rows with comments that are deleted\n",
    "for subreddit in subreddits:\n",
    "    pattern_del = \"\\[deleted\\]\"\n",
    "    filter = subreddit['comment_text'].str.contains(pattern_del)\n",
    "    subreddit = subreddit[~filter]\n",
    "    subreddit = subreddit.sample(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update clean_text function\n",
    "def clean_text(raw_text):\n",
    "    \n",
    "    #convert all characters to lowercase\n",
    "    text = raw_text.lower()\n",
    "    \n",
    "    #remove http url links\n",
    "    text = re.sub('http.*.com', '',text)\n",
    "    \n",
    "    #remove subreddit references\n",
    "    text = re.sub('r\\/.*', '', text)\n",
    "    \n",
    "    #remove new line '\\n'\n",
    "    text = re.sub('\\n', ' ', text)\n",
    "        \n",
    "    #convert accented characters to ASCII characters\n",
    "    text = unidecode.unidecode(text)\n",
    "    \n",
    "    #expand contractions\n",
    "    text = contractions.fix(text)\n",
    "    \n",
    "    # Fix other contractions / possessive\n",
    "    text = text.replace(\"'s\", '')\n",
    "    \n",
    "    #remove special characters and punctuations\n",
    "    text = re.sub('[^A-Za-z0-9]+', ' ', text)\n",
    "    \n",
    "    #remove numbers\n",
    "    text = re.sub('\\d+', '', text)\n",
    "    \n",
    "    #remove extra white space\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    \n",
    "    #remove stop words\n",
    "    cleaned_text = remove_stopwords(text)\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()\n",
    "  \n",
    "# Define function to lemmatize each word with its POS tag\n",
    "  \n",
    "def lemmatize_word(tagged_token):\n",
    "    \"\"\" Returns lemmatized word given its tag\"\"\"\n",
    "    root = []\n",
    "    for token in tagged_token:\n",
    "        tag = token[1][0]\n",
    "        word = token[0]\n",
    "        if tag.startswith('J'):\n",
    "            root.append(lemma.lemmatize(word, wordnet.ADJ))\n",
    "        elif tag.startswith('V'):\n",
    "            root.append(lemma.lemmatize(word, wordnet.VERB))\n",
    "        elif tag.startswith('N'):\n",
    "            root.append(lemma.lemmatize(word, wordnet.NOUN))\n",
    "        elif tag.startswith('R'):\n",
    "            root.append(lemma.lemmatize(word, wordnet.ADV))\n",
    "        else:          \n",
    "            root.append(word)\n",
    "    return root\n",
    "\n",
    "def lemmatize_doc(document):\n",
    "    \"\"\" Tags words then returns sentence with lemmatized words\"\"\"\n",
    "    lemmatized_list = []\n",
    "    tokenized_sent = sent_tokenize(document)\n",
    "    for sentence in tokenized_sent:\n",
    "        no_punctuation = re.sub(r\"[`'\\\",.!?()]\", \" \", sentence)\n",
    "        tokenized_word = word_tokenize(no_punctuation)\n",
    "        tagged_token = pos_tag(tokenized_word)\n",
    "        lemmatized = lemmatize_word(tagged_token)\n",
    "        lemmatized_list.extend(lemmatized)\n",
    "    return \" \".join(lemmatized_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine clean_text and lemmatize_doc functions\n",
    "def preprocess(data):\n",
    "    data['cleaned_text'] = data['comment_text'].apply(lambda x: clean_text(x))\n",
    "    data['lemmatized_text'] = data['cleaned_text'].apply(lambda x: lemmatize_doc(x))\n",
    "    cleaned_data = data\n",
    "    return(cleaned_data)\n",
    "\n",
    "for subreddit in subreddits:\n",
    "    preprocess(subreddit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model\n",
    "lr_model = pickle.load(open('lr_model.sav', 'rb'))\n",
    "\n",
    "def subreddit_toxicity_percent(df):\n",
    "    sub_text= df.lemmatized_text.values\n",
    "    sub_preds = lr_model.predict(sub_text)\n",
    "    return round(sub_preds.sum()/sub_preds.shape[0],2)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subreddits</th>\n",
       "      <th>Toxicity Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KotakuInAction</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MensRights</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NoFap</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TrollXChromosomes</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TumblrInAction</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Subreddits  Toxicity Percentage\n",
       "0     KotakuInAction                 38.0\n",
       "1         MensRights                 41.0\n",
       "2              NoFap                 31.0\n",
       "3  TrollXChromosomes                 40.0\n",
       "4     TumblrInAction                 41.0"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {'Subreddits': ['KotakuInAction', 'MensRights', 'NoFap', 'TrollXChromosomes', 'TumblrInAction'], \n",
    "     'Toxicity Percentage': [subreddit_toxicity_percent(KotakuInAction), subreddit_toxicity_percent(MensRights), subreddit_toxicity_percent(NoFap), subreddit_toxicity_percent(TrollXChromosomes), subreddit_toxicity_percent(TumblrInAction)]}\n",
    "subreddit_score = pd.DataFrame(data=data)\n",
    "subreddit_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
